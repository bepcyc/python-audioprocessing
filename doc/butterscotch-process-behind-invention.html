<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>

    <meta http-equiv="Content-Type" content="text/html;charset=utf-8">

    <meta name="generator" content="Plone - http://plone.org">

    <!-- Internet Explorer fix, forces IE8 into newest possible rendering
         engine even if it's on an intranet. This has to be defined before any
         script/style tags. -->
    <meta http-equiv="X-UA-Compatible" content="IE=edge">


    <base href="http://oldzope.rudd-o.com/Rudd-O.com/new-projects/python-audioprocessing/documentation/manuals/butterscotch-the-process-behind-its-invention/">
    <!--[if lt IE 7]></base><![endif]-->


    <title>Butterscotch: the process behind its invention — Rudd-O.com</title>


    <meta content="A practical application of FFTs and statistical analysis on music." name="DC.description">
    <meta content="A practical application of FFTs and statistical analysis on music." name="description">
    <meta content="text/plain" name="DC.format">
    <meta content="Reference Manual" name="DC.type">
    <meta content="RuddO" name="DC.creator">
    <meta content="2008-12-04 12:08:23" name="DC.date.modified">
    <meta content="2008-12-04 12:02:12" name="DC.date.created">
    <meta content="en" name="DC.language">


    <link rel="shortcut icon" type="image/x-icon" href="http://oldzope.rudd-o.com/Rudd-O.com/favicon.ico">


    <link rel="home" href="http://oldzope.rudd-o.com/Rudd-O.com" title="Front page">

    <link rel="contents" href="http://oldzope.rudd-o.com/Rudd-O.com/sitemap" title="Site Map">


    <link rel="search" href="http://oldzope.rudd-o.com/Rudd-O.com/search_form" title="Search this site">


    <!-- Disable IE6 image toolbar -->
    <meta http-equiv="imagetoolbar" content="no">


</head>

<body class="section-new-projects template-referencemanual-all-pages" dir="ltr">
<div id="visual-portal-wrapper">


<table id="portal-columns">
<tbody>
<tr>


<td id="portal-column-content">


<div class="">


<div id="region-content" class="documentContent">


<a name="documentContent"></a>


<div id="content">

<div>

<div>


<h1>
    <img src="http://oldzope.rudd-o.com/Rudd-O.com/referencemanual_icon.gif" alt="" title="Reference manual" height="16"
         width="16">
    Butterscotch: the process behind its invention
</h1>


<div class="documentDescription">A practical application of FFTs and statistical analysis on music.</div>


<div class="referenceManualCollation depth-1">

    <h1>
        1.
        The problem
    </h1>

    <div class="documentDescription">Collection-based players help you organize your music. But they can help even
        more.
    </div>

    <div id="bodyContent">


        <ul>
            <li>Twenty thousand songs.
            </li>
            <li>Hundreds of full albums and compilations.
            </li>
            <li>Half a thousand duplicates.
            </li>
        </ul>
        <p>
            As an user of <span class="link-external"><a class="ext-link" href="http://amarok.kde.org/"><span
                class="icon">the single greatest music player ever invented</span></a></span>, and an avid collector of
            music, our hypothetical (yet very real) music fan has a problem however you slice it.</p>

        <p>
            Should he want to remove duplicates, he has to do so manually, relying
            on (oftentimes incorrect) meta information. 'Manually', in this
            context, means manually trolling through his collection, and cueing
            songs by the same artist with the same or similar names (in the best
            case scenario). No one has time to troll through twenty thousand songs,
            and troll again every time a track is incorporated to his collection.
            And once a duplicate has been spotted, which one of the two (...N?)
            songs should be deleted?</p>

        <p>Should he prefer to keep his full albums and compilations instact,
            the problem is statistical. Collection-based players can tell you which
            song is your favorite by virtue of tracking how many times and how
            frequently you listen to particular songs. Two files from different
            albums with the same audio material (same edit of the song) should
            therefore share the rating and score. In practice, listening to copy A
            of track X does not raise the score of copy B of the same track X,
            rendering robotic "favorite lists" moot. This is a problem because
            contemporary music players such as Amarok can fill up portable music
            devices automatically according to a set of criteria including
            user-supplied ratings and automatically-computed scores. This means
            that duplicate tracks find their way into portable music devices, and
            real favorites don't. The user must thus revert to manual management of
            his portable music collection.</p>

        <p>For both user stories, acquisition of new music exacerbates the
            problem. What's the likelihood that a track X is already present in a
            large collection? If so, what's the point of adding another copy of X
            to it?</p>

        <p>In summary: duplicate tracks pose practical problems for people who
            just want to listen to their favorite music. Either the user wants to
            delete duplicates, or he wants the scores and ratings accurately
            tracked.</p>

        <p>
            Hmmmm... aren't computers supposed to be good at crunching data and making decisions?</p>


    </div>


</div>


<div class="referenceManualCollation depth-1">

    <h1>
        2.
        Fundamental principles and goals
    </h1>

    <div class="documentDescription">What we are trying to accomplish here.</div>

    <div id="bodyContent">


        <p>
            What we want is a smart computer program that is able to identify
            duplicate songs. This document does not concern itself with the user
            interface of said program -- it may be in the forms of:</p>
        <ul>
            <li>a standalone tool that will present the user with duplicates and enable him to quickly weed out
                duplicates, or
            </li>
            <li>a background service that will transparently fix music
                players' statistics so they present a coherent and accurate view of the
                music fan's preferences.
            </li>
        </ul>
        <p>The problem is not really how the program should look
            like, but how reliably it identifies duplicates. For the record, I
            chose the second path for myself.</p>

        <h3 id="Sowhatsaduplicate">So what's a duplicate?<span class="link-external"><a title="Link to this section"
                                                                                        class="anchor"
                                                                                        href="http://projects.rudd-o.com/python-audioprocessing/wiki/FourierAndCorrelationsAppliedToSongAnalysis#Sowhatsaduplicate">
            ¶</a></span></h3>

        <p>
            For the purposes of this document, X is a duplicate of Y if:</p>
        <ul>
            <li>X is the same song edit/mix as Y,
            </li>
            <li>X and Y are indistinguishable aside from linear volume or minor EQ changes,
            </li>
            <li>neither X or Y are parts of a megamix (i.e., crossfaded in
                mixes intended to be listened from start to end) -- arguably, the user
                would like to either keep those songs intact or would not consider them
                to be the same for ratings and scoring purposes.
            </li>
        </ul>
        <p>In other words, if X and Y are the same song, the same
            edit by the same artist, but they were released in different albums,
            they should be recognized as duplicates. Particularly regardless of:</p>
        <ul>
            <li>incompletenesss: if X is incomplete, Y and X should still match
                so the user can make an informed decision to kill or replace the
                incomplete song.
            </li>
            <li>different tempos: common sense dictates that re-tempoed songs
                should not share the same ratings and scores, nor would anyone
                recognize re-tempoed songs as the same content of the original ones.
            </li>
            <li>different encoding quality: if X and Y are the same song but
                X was encoded at a considerably lower bitrate than Y, X and Y should
                still match so the user can remove or ban the lower-quality version of
                the song.
            </li>
        </ul>


    </div>


</div>


<div class="referenceManualCollation depth-1">

    <h1>
        3.
        The anatomy of a hypothetical workable solution
    </h1>

    <div class="documentDescription">What can get us to our goal.</div>

    <div id="bodyContent">


        <p>
            All techniques to analyze and compare songs boil down to this:</p>
        <ul>
            <li>Use an algorithm to compute a practical, usable representation
                of each song that preserves the features one wants to compare against.
            </li>
            <li>Use a different algorithm to compare the representations of
                each song with the representations of every other available song. The
                comparison should map to a single value that will represent "duplicate
                / not duplicate" for the purposes of automated decision-making.
            </li>
        </ul>
        <h3 id="Existingtechniques">Existing techniques<span class="link-external"><a title="Link to this section"
                                                                                      class="anchor"
                                                                                      href="http://projects.rudd-o.com/python-audioprocessing/wiki/FourierAndCorrelationsAppliedToSongAnalysis#Existingtechniques">
            ¶</a></span></h3>

        <p>
            There are a variety of techniques that can be used to weed duplicates out. Here are some of them, and why
            they are ruled out:</p>
        <ul>
            <li>Hashing and comparison of on-disk file data. Effective to
                discover exact duplicates. However, sensitive to minute changes in
                on-disk representation that amount to no audible difference.
            </li>
            <li>Hashing and comparison of raw, decoded data. Effective to
                discover duplicates with different metadata. Sensitive to encoding
                parameters.
            </li>
            <li>Identification and comparison of melody sequences and subsets
                thereof. Not useful, since we want to distinguish different cuts/mixes
                of the same track.
            </li>
        </ul>
        <p>What we need is some form of representation and
            comparison that will result in high values for songs that sound
            "extremely alike" (i.e. vitually indistinguishable), and low values for
            songs that are perceptually different. We also need that both processes
            (the feature computation/extraction and the comparison) be fast.
            Especially the comparison, since it is an O(n*n) process.</p>

        <p>
            Turns out, there isn't much new science to do.</p>


    </div>


</div>


<div class="referenceManualCollation depth-1">

    <h1>
        4.
        Designing Butterscotch
    </h1>

    <div class="documentDescription">The specific problems we need to solve in the domain of audio.</div>

    <div id="bodyContent">


        <p>
            Butterscotch is a combination feature extraction and comparison
            algorithm that combines well-studied techniques for audio analysis in
            the digital domain.</p>

        <p>
            I knew two things:</p>
        <ul>
            <li>duplicate songs have nearly identical spectrographs,
            </li>
            <li>duplicate songs may have different start times (song X from disc A may start half a second later than
                song Y from disc B).
            </li>
        </ul>
        <h3 id="TheButterscotchalgorithms">The Butterscotch algorithms<span class="link-external"><a
                title="Link to this section" class="anchor"
                href="http://projects.rudd-o.com/python-audioprocessing/wiki/FourierAndCorrelationsAppliedToSongAnalysis#TheButterscotchalgorithms">
            ¶</a></span></h3>

        <p>
            The problem was easy (or so I thought):</p>
        <ul>
            <li>Find the "onset of audio" mark in the song. I briefly
                experimented and settled on 2dB RMS increases in 11 ms windows as
                reliable markers of "onset of audio".
            </li>
            <li>Read a preset, meaningful amount of data from that point onwards, and chunk it in 10-second blocks. I
                used 12 blocks.
            </li>
            <li>For each block, compute FFT representations (spectrum
                analyses). I used 64-point real FFTs (32 non-DC resulting bands) to
                begin with. The high half of the bands was to be thrown out because
                poor encoders also throw out lots of high frequencies, so that half
                wasn't going to be meaningful for this process.
            </li>
            <li>Compare the FFT of each block in X to the FFT of the
                corresponding block in Y. using some sort of function. My experiments
                showed that correlation was the optimum comparison process, and that
                correlation coefficients from each comparison of two songs should be
                averaged.
            </li>
        </ul>
        <p>Ideally, there is a correlation threshold (or band) above
            which "X and Y are duplicate", and below which the opposite holds. This
            should hold true for all songs in a collection, so if correlating two
            non-duplicates results in a higher value than the lowest correlation of
            two real duplicate songs, the algorithm is not very good. That is what
            we refer to as a "false positive" or "false negative". depending on
            which threshold is chosen.</p>

        <h3 id="Correlationbetweenspectrumsvs.time-basedbandcorrelations">Correlation between spectrums vs. time-based
            band correlations<span class="link-external"><a title="Link to this section" class="anchor"
                                                            href="http://projects.rudd-o.com/python-audioprocessing/wiki/FourierAndCorrelationsAppliedToSongAnalysis#Correlationbetweenspectrumsvs.time-basedbandcorrelations">
                ¶</a></span></h3>

        <p>Having watched too many spectrographs, I started with the assumption
            that the frequency representations of duplicate songs were strongly
            related. That is, correlating the intensities of song X and song Y in a
            particular block of audio should yield me a near-perfect correlation.
            That assumption turned out to be very, very wrong, throwing too many
            false positives and negatives. In other words, many songs share
            unbelievably similar intensities in each frequency band for blocks of
            audio, while imperceptible EQ differences in otherwise "identical"
            songs threw correlation in the tank.</p>

        <p>However, what did turn out to be correct is that correlating time
            series from corresponding bands was a much, much better predictor of
            similarity. In other words, let's assume that the feature extractor
            creates a two-dimensional array, where the rows are to be interpreted
            as spectrums, and the columns are time series for a particular band. I
            was trying to correlate row-by-row, when in reality column-by-column
            was much, much better.</p>

        <p>I presume there is a common sense explanation for this: as long as
            the "onset of audio" is accurate, an increase in intensity from time
            block N to time block N+1 for any particular band in song X is
            obviously going to be accompanied by the same linear increase in
            intensity in song Y. In other words, if there's a bass playing in
            seconds 26-36, that will hold true for both X and Y.</p>

        <h3 id="Linearintensitiesvs.dBvalues">Linear intensities vs. dB values<span class="link-external"><a
                title="Link to this section" class="anchor"
                href="http://projects.rudd-o.com/python-audioprocessing/wiki/FourierAndCorrelationsAppliedToSongAnalysis#Linearintensitiesvs.dBvalues">
            ¶</a></span></h3>

        <p>I briefly toyed with feature extractions of decibel-based values. My
            experiments showed that using dBs instead of raw values hurt
            correlation significantly, muddying the results.</p>

        <h3 id="CorrelationtimeisinOnn">Correlation time is in O(n*n)<span class="link-external"><a
                title="Link to this section" class="anchor"
                href="http://projects.rudd-o.com/python-audioprocessing/wiki/FourierAndCorrelationsAppliedToSongAnalysis#CorrelationtimeisinOnn">
            ¶</a></span></h3>

        <p>My experiments showed me that feature extractions of anything below
            32 bands are prone to false positives and false negatives. Problem is,
            correlating anything above that is prohibitively expensive in my
            computer. Correlating features of one song against a thousand other
            songs took over 25 seconds in my machine. Divide that time over 1000,
            and multiply it by 20000 * 20000 and you get an idea of the magnitude
            of the problem.</p>

        <p>In other words, duplicate the number of bands and you increase
            correlation precision by a just bit, but you just quadrupled the time
            you need to correlate your entire collection.</p>

        <h3 id="Logarithmicallyaveragedbands">Logarithmically averaged bands<span class="link-external"><a
                title="Link to this section" class="anchor"
                href="http://projects.rudd-o.com/python-audioprocessing/wiki/FourierAndCorrelationsAppliedToSongAnalysis#Logarithmicallyaveragedbands">
            ¶</a></span></h3>

        <p>What if, instead of performing 2048-point FFTs (therefore 1024
            correlations for each song pair) and then averaging the correlation
            coefficients, we only had to perform 10? Would that not speed up the
            correlation process dramatically?</p>

        <p>
            Turns out, it does. Not only that, but it improves correlations superbly. When we use logarithmically
            averaged bands.</p>

        <p>I use a simple process where we divide the resulting bands from the
            feature extraction in half, average the right half (resulting in one
            wide band for the high frequencies) and perform the same process again,
            recursively, for the left half. I can get away with this because the
            difference between 440 and 441 Hz is more significant that the
            difference between 4400 and 4401 Hz.</p>

        <p>
            Two birds with one stone:</p>
        <ol>
            <li>Correlation speeds up by a factor of 100.
            </li>
            <li>Insignificant differences in the frequency domain get averaged out. However, the correlation in the time
                domain stays the same.
            </li>
        </ol>


    </div>


</div>


<div class="referenceManualCollation depth-1">

    <h1>
        5.
        Tuning the parameters
    </h1>

    <div class="documentDescription">What are the ideal parameters to our generic algorithm?</div>

    <div id="bodyContent">


        <p>
            We need to find the ideal feature characteristics for the Butterscotch
            feature extractor. Ideal, in this sense, is defined by how big is the
            difference between the highest correlation for non-identical songs, and
            the lowest correlation for identical songs, so that we can reliably set
            a threshold and trust that the threshold will be enough to make an
            automated decision. The parameters can be tuned along the following
            axes of the problem space:</p>
        <ol>
            <li>Number of bands (in other words: number of points in the FFT)
            </li>
            <li>Linear bands (constant width) vs. logarithmic bands (constant Q)
            </li>
            <li>Number of blocks.
            </li>
            <li>Block length (in seconds)
            </li>
        </ol>
        <p>
            Here is how the process of tuning the parameters went along.</p>

        <h3 id="Productionofalistofsongs">Production of a list of songs<span class="link-external"><a
                title="Link to this section" class="anchor"
                href="http://projects.rudd-o.com/python-audioprocessing/wiki/FourierAndCorrelationsAppliedToSongAnalysis#Productionofalistofsongs">
            ¶</a></span></h3>

        <p>I used Amarok to select a fairly manageable number of tracks, some
            of which were positively duplicates. 500 songs were my test sample.</p>

        <h3 id="Productionofapositivelistofduplicates">Production of a positive list of duplicates<span
                class="link-external"><a title="Link to this section" class="anchor"
                                         href="http://projects.rudd-o.com/python-audioprocessing/wiki/FourierAndCorrelationsAppliedToSongAnalysis#Productionofapositivelistofduplicates">
            ¶</a></span></h3>

        <p>A list of duplicates contains groups of songs that are identical,
            determined by manual hearing of all tracks. This list was used to:</p>
        <ol>
            <li>separate known duplicates in printouts,
            </li>
            <li>visually distinguish them in 3D scatter graphs.
            </li>
        </ol>
        <p>
            Coming up with this list was harder work than coming up with the initial playlist.</p>

        <h3 id="Featureextraction">Feature extraction<span class="link-external"><a title="Link to this section"
                                                                                    class="anchor"
                                                                                    href="http://projects.rudd-o.com/python-audioprocessing/wiki/FourierAndCorrelationsAppliedToSongAnalysis#Featureextraction">
            ¶</a></span></h3>

        <p>The feature extraction was done by the command
            butterscotch-batchanalyze. To explore different axes of the problem, I
            used features of the software that allowed me to reduce the resolution
            of each feature along various axes. Thus, after generating N-block
            samples of linear bands, I could generate features of N/2-block samples
            or features with logarithmic bands at nary an additional cost.</p>

        <p>Then, I correlated the results of each experiment in batches. I kept
            the results of the feature extractions and correlations on separate
            files on disk, which allowed me to quickly switch between data sets and
            plot them simultaneously onscreen.</p>

        <h3 id="Visualizingtheresultseffectively">Visualizing the results effectively<span class="link-external"><a
                title="Link to this section" class="anchor"
                href="http://projects.rudd-o.com/python-audioprocessing/wiki/FourierAndCorrelationsAppliedToSongAnalysis#Visualizingtheresultseffectively">
            ¶</a></span></h3>

        <p>I plotted the results of several different feature extraction batch
            processes (with different parameters), in 3D space -- the Z axis was
            the correlation. The positive list of duplicates is shown with a
            different type of marker that makes it easy to spot. Also, the bottom
            half of the positive matches and the top half of the rest are displayed
            with the purposes of discovering duplicates missed in the production of
            the positive list, and comparing raw correlation coefficients. I had to
            trim the tentative negative matches to about 2000 points or so, because
            my computer was unable to handle larger data volumes in an effective
            manner.</p>

        <p>This graph invariably looked like two stacked clouds: the one on top
            always a cluster of positively duplicate songs, and the one at the
            bottom a cluster of positively nonduplicates, separated by a clear "no
            man's land" where very few correlations were plotted. This graph was
            chosen because it was a very effective representation to quickly spot
            false negatives (known duplicates creeping down into the nonduplicate
            song cloud) and false positives (nonduplicates creeping into the known
            duplicate cloud).</p>

        <p>
            With this tool, I could test my experiments effectively.</p>

        <h3 id="Resultsoftheexperiments:observations">Results of the experiments: observations<span
                class="link-external"><a title="Link to this section" class="anchor"
                                         href="http://projects.rudd-o.com/python-audioprocessing/wiki/FourierAndCorrelationsAppliedToSongAnalysis#Resultsoftheexperiments:observations">
            ¶</a></span></h3>
        <ul>
            <li>Logarithmic bands are more effective and computationally much.
                much faster in correlation time than their linear band counterparts.
                The logarithmic averaging process is a bargain.
            </li>
            <li>The optimum number of points for the FFT is 512, which
                results in 256 linear bands up to 22.05 KHz, which results in 128 bands
                up to 11.025 KHz, which then (as pointed earlier) get averaged down to
                8 logarithmic bands whose center points lie between 86 and 8311 Hz. The
                command line options for this result are -num-bands 256 --log-bands.
                This causes the wav_butterscotch() function to be invoked with
                num_bands=256, and the results munged with the halve_highest_freq() and
                as_log_bands() methods.
            </li>
            <li>Reducing the number of blocks from 2 minutes to 1 minute (6
                10-second blocks) threw a wild array of false positives for songs whose
                bridges were different later on in the songs.
            </li>
            <li>Reducing block resolution to 20 seconds while moving to 6
                blocks per song produced a negative gap between the lowest positively
                duplicates and the highest positively nonduplicates. On the contrary,
                increasing block resolution to 5 seconds and block count to 24 actually
                increased the gap almost 0.03 between the strongest nonduplicate
                correlation and weakest duplicate correlation. 30 4-second blocks were
                even better, increasing the gap by more than 0.06, and leading to very
                distinct clouds.
            </li>
        </ul>
        <p>My conclusions point to these results as the best: 256
            bands, half-spectrum, logarithmically averaged (eight total bands)
            sampled 30 times for 4 seconds each.</p>


    </div>


</div>


<div class="referenceManualCollation depth-1">

    <h1>
        6.
        Unsolved problems
    </h1>

    <div class="documentDescription">Nitpicks and problems that need solving to perfect the algorithm.</div>

    <div id="bodyContent">


        <ol>
            <li>Otherwise exactly identical songs with different lyrics (say,
                French version vs. English version) performed by the same performer
                correlate very highly. Obviously, those songs aren't "duplicates" under
                the Butterscotch definition.
            </li>
        </ol>


    </div>


</div>


</div>

</div>
</div>


</div>

</div>


</td>


</tr>
</tbody>
</table>


</div>


</body>
</html>
